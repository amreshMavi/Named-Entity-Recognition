{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d6c94c-d9dc-466c-90ed-c06231fb0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "data = pd.read_csv('./data/ner_dataset.csv', encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78bb394f-d44b-49b3-9921-0a07b7ebd866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e69b6d-3d01-4d3a-bed5-8d6bbede9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c02451-6162-4de2-a182-81803dc84adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfec66a-ca63-4891-aa5e-65d9d46bd259",
   "metadata": {},
   "source": [
    "In the data, we can see that the words are broken into columns which will represent our feature X, and the Tag column in the right will represent our label Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217eed5-1577-4d35-b6ff-5cdbc339fc97",
   "metadata": {},
   "source": [
    "Data Preparation for Neural Networks\n",
    "I will train a Neural Network for the task of Named Entity Recognition (NER). So we need to do some modifications in the data to prepare it in such a manner so that it can easily fit into a neutral network. I will start this step by extracting the mappings that are required to train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfde053-7009-4981-981a-b9c21745d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "\n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "\n",
    "    idx2tok = {idx:tok for idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad4d33-64ce-4a30-84e1-4feef0c205d5",
   "metadata": {},
   "source": [
    "transform the columns in the data to extract the sequential data for our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110d24d8-a5f1-4f6c-9d0e-8e18342914e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "data_fillna = data.ffill()\n",
    "# Groupby and collect columns\n",
    "data_group = data_fillna.groupby(\n",
    "    ['Sentence #'],as_index=False\n",
    ")[['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx']].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648b28a-9cd1-466e-b56e-66f21431a235",
   "metadata": {},
   "source": [
    "Now I will split the data into training and test sets. I will create a function for splitting the data because the LSTM layers accept sequences of the same length only. So every sentence that appears as integer in the data must be padded with the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7faf244-ea72-4dcb-a4ac-b909ab5d2044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 32372 \n",
      "train_tokens length: 32372 \n",
      "test_tokens length: 4796 \n",
      "test_tags: 4796 \n",
      "val_tokens: 10791 \n",
      "val_tags 10791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def get_pad_train_test_val(data_group, data):\n",
    "\n",
    "    # get max token and tag length\n",
    "    n_token = len(list(set(data['Word'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "\n",
    "    # Pad tokens (X var)\n",
    "    tokens = data_group['Word_idx'].to_list()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value=n_token - 1)\n",
    "\n",
    "    # Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['Tag_idx'].to_list()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = to_categorical(pad_tags, num_classes=n_tags)\n",
    "\n",
    "    # split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2026)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_, tags_, test_size=0.25, train_size=0.75, random_state=2026)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "        '\\nval_tokens:', len(val_tokens),\n",
    "        '\\nval_tags', len(val_tags),\n",
    "    )\n",
    "\n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
    "    \n",
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f67a0fc9-12d4-4636-a92c-23ee48b9e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to consistent types\n",
    "X_train_final = np.array(train_tokens).astype('int32')\n",
    "y_train_final = np.array(train_tags).astype('float32')\n",
    "\n",
    "# This handles the internal memory \"Graph\" more reliably\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_final, y_train_final))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "X_val_final = np.array(val_tokens).astype('int32')\n",
    "y_val_final = np.array(val_tags).astype('float32')\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_final, y_val_final)).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bdb93-c79f-4e2b-9ca4-c890f31fd2f7",
   "metadata": {},
   "source": [
    "Training Neural Network for Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e904840-b2bb-4dfc-8a4b-2e7733a842d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412cd7d7-5b8b-4e5e-849b-44355a9f0a7f",
   "metadata": {},
   "source": [
    "The layer below will take the dimensions from the LSTM layer and will give the maximum length and maximum tags as an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8356fe48-cea8-4a5a-ad5a-ea2c36aaf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(list(set(data['Word'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].to_list()])\n",
    "n_tags = len(tag2idx)\n",
    "n_words = len(token2idx) + 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56166333-2d6a-42a8-92fc-1f36033640cd",
   "metadata": {},
   "source": [
    "Now I will create a helper function which will help us in giving the summary of every layer of the neural network model for Named Entity Recognition (NER):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71dadb39-f4af-4bf5-a227-ef16a6e86644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "def get_bilstm_lstm_model():\n",
    "    \n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=n_words, output_dim=output_dim, input_length=104))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2)))\n",
    "\n",
    "    # Dropout layer\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Add Dense layer\n",
    "    model.add(Dense(n_tags, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b8d75bd-e806-4ff4-9a38-59571e5689d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 104, 64)           2251456   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 104, 256)         197632    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 104, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 104, 256)          0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 104, 17)           4369      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,847,697\n",
      "Trainable params: 2,847,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "506/506 [==============================] - 52s 80ms/step - loss: 0.1387 - accuracy: 0.9713 - val_loss: 0.0544 - val_accuracy: 0.9841\n",
      "Epoch 2/3\n",
      "506/506 [==============================] - 41s 82ms/step - loss: 0.0413 - accuracy: 0.9879 - val_loss: 0.0306 - val_accuracy: 0.9912\n",
      "Epoch 3/3\n",
      "506/506 [==============================] - 40s 78ms/step - loss: 0.0251 - accuracy: 0.9928 - val_loss: 0.0261 - val_accuracy: 0.9924\n"
     ]
    }
   ],
   "source": [
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "    \n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec60fbed-3d9d-4915-9c56-fcbd0d453ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hi, My name is \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mavi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " .  I am from \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " .  I want to work with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " .  \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is My Inspiration</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "clean_text = 'Hi, My name is Mavi \\n I am from India \\n I want to work with Google \\n Steve Jobs is My Inspiration'.replace('\\n', '. ')\n",
    "text = nlp(clean_text)\n",
    "displacy.render(text, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b10f9bf1-bc34-4070-bc5f-eeea0d22544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04619466-0e18-47b4-a309-788986d15175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"artifacts/model.h5\")\n",
    "\n",
    "pickle.dump(token2idx, open(\"artifacts/word2idx.pkl\", \"wb\"))\n",
    "pickle.dump(idx2tag, open(\"artifacts/idx2tag.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543661a-fe89-4bf7-8be6-9320ea533fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
